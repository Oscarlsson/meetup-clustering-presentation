{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:999349cd854fe2cac78f46ebdf933a8de9eeacdaa26b9cd187d5c1d5afbe9a31"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "<img src=\"fig/Spotify_Logo_RGB_Black.png\" style=\"height:auto; width:auto; max-width:300px; max-height:300px; position:left;\"/>\n",
      "\n",
      "## Welcome to the Machine Learning meetup!\n",
      "\n",
      "\n",
      "Todays agenda:\n",
      "\n",
      "\n",
      "** *18:15 - 18:45:*\n",
      "Introduction to clustering and feature engineering**\n",
      "\n",
      "Short break, refill drinks, greet & meet etc.\n",
      "\n",
      "*19:00 - 19:30:*\n",
      "Introduction to data (mining) analysis in Python\n",
      "\n",
      "*19:30: *\n",
      "Hang around and discuss clustering and machine learning. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 400%\"> Who is it that we have here today? </h1>\n",
      "<br>\n",
      "<br>\n",
      "<br>\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Round the table\n",
      "\n",
      "My name is Oscar Carlsson\n",
      "* Data Engineer at Spotify\n",
      "* Today I do statstical analysis, run A/B test and anything related to our huge amount of data.\n",
      "* My msc thesis was a cluster analysis on spotify. \n",
      "\n",
      "\n",
      "I will try to answer your questions now or in the break\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 400%\"> Introduction to clustering and feature enginneering <h1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "This is the first of two presentations - an introduction to clustering and feature engineering\n",
      "\n",
      "and the later will be more code, both intro to python data analysis and clustering\n",
      "Get a feeling of how you could do it"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Outline of the todays talk\n",
      "* ## The problem\n",
      "* ## Clustering algorithms\n",
      "* ## Evaluation of the clustering\n",
      "* ## Machine Learning at Spotify"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\"> The problem of clustering <h1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "So, what is clustering?\n",
      "\n",
      "Clustering is the task of grouping a set of object in such a way \n",
      "\n",
      "that objects in the same group are more similar to each other\n",
      "\n",
      "than to those in other groups.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\"> Clustering is the task of **grouping** a set of object in such a way that objects in the same group are more **similar** to each other than to those in other groups. <h1 style=\"font-size: 400%\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "What is interesting in this is the resulting groups. \n",
      "What objects ended up in the same cluster, and why? and what does that tell us?\n",
      "\n",
      "\n",
      "two important words - ** grouping ** and **similiar**\n",
      "\n",
      "# **Grouping** - group = cluster\n",
      "Each group could represent or tell us about something the objects share, some **characteristic**.\n",
      "\n",
      "\n",
      "# **Similiar - two arbitrary objects**.\n",
      "\n",
      "In clustering, how we define two objects to be similar has great importance to the output.\n",
      "So, how do define that two \u201cobjects\u201d or \u201csamples\u201d in our data is similar? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\"> Grouping similar objects <h1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# The objects could be text documents (news articles)\n",
      "# And the grouping could be the theme of the text documents"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "I said that what is interesting with clustering is the **resulting groups**, the output of a clustering algortihm or model.\n",
      "\n",
      "And the output: for each sample in our data, **to what group does it belong?**\n",
      "Why those objects? Why does groups?\n",
      "\n",
      "In our example the output would be a assignment, for each text doucment, to which theme it belongs.\n",
      "Then the output is a **hidden structure** in our data. Nothing that is clearly stated, maybe obvious for a human, \n",
      "but not for a computer.\n",
      "\n",
      "Basically, **we want the computer to find and tells us about a hidden structure in our unlabeled data (un grouped)**\n",
      "\n",
      "This is the same explanaition used to describe **Unsupervised learning**.\n",
      "Clustering is unsupervised.\n",
      "But before we talk about unsupervised - I would like to mention Supervised Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\">  Supervised Learning <h1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "And i guess the most of you are familiar with the terms Supervised Learning and Unsupervised learning. \n",
      "Explained just very brielfy supervised learning is when \n",
      "\n",
      "we know know a **structure (labeling of data)** and want to \n",
      "\n",
      "**train a model**\n",
      "\n",
      "*so that it can **reproduce this structure on unknown data**. \n",
      "\n",
      "If you have heard about **classification** we train a classifier on **training data** and categorize new samples with the model.\n",
      "So we have the data: X\n",
      "and we have the corresponding labels: Y\n",
      "\n",
      "But training data is expensive. Not many have this assignment, it is much more common with just X\n",
      "- Just Documents - Just a lot of data.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<center><h1 style=\"font-size: 300%\"> We have the data X and we have the labels Y <h1></center>\n",
      "<center><h1 style=\"font-size: 300%\"> (DOCUMENTS and CATEGORYS) <h1></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\"> Unsupervised Learning <h1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<center><h1 style=\"font-size: 300%\"> The Data X only <h1></center>\n",
      "<center><h1 style=\"font-size: 300%\"> (DOCUMENTS ONLY) <h1></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "We have no Y. \n",
      "\n",
      "To let the machine (or computer) learn and find this Y that is explaining what we asked the computer of\n",
      "\n",
      "We need to **feed it** and **help it** by giving the **relevant** data.\n",
      "\n",
      "Obviously? No! Not always\n",
      "And what is relevant is what are going to put in X. The data.\n",
      "Finding similar artist: Based on what music they produce? On who is listening to them?\n",
      "Depending on what we use as the data\n",
      "\n",
      "\n",
      "And in the field of machine learning there is a rather **unglourious** but extremly **necessary** task called feature **engineering**.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\"> Feature Engineering <h1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Our dataset $$\n",
      "X = \n",
      "  \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \n",
      "$$\n",
      "\n",
      "Each sample\n",
      "$$x_1 =   \\begin{pmatrix} f_1, & f_2, & f_3, & f_4, & \\dots, & f_n \\end{pmatrix}^T $$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "We are not talking about **software features** but feature as some **PROPERTY** we observe in our data.\n",
      "\n",
      "One feature is an **individual measurable property observed**.\n",
      "\n",
      "Big X is a feature matrix of size n_samples times n_features\n",
      "\n",
      "small x is a **vector of features**: refered to as **Feature vector**\n",
      "\n",
      "\n",
      "\n",
      "The usual assumption of a algorithm is that the inpux $X$ is **NUMERICAL** "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "<center> <h2 style=\"font-size: 200%\"> \n",
      "Define X means  <br>\n",
      "define each $x_i$ <br>\n",
      "means decide each $f_i$ <br>\n",
      "= <br>\n",
      "Feature Engineering\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# We have $n$ DOCUMENTS\n",
      "# Each document $x_i$ is a numerical vector\n",
      "# We design the meaning of each $f_i$\n",
      "<br>\n",
      "# Features: The words? The datetime? The font used?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Or if what we are clustering is text, such as music lyrics,\n",
      "we can use the **\u201cBAG-OF-WORDS MODEL\u201d** to present each lyrics.\n",
      "\n",
      "Then the** occurence of each word** is used as a **feature**.\n",
      "More on that in the demo after the break.\n",
      "\n",
      "\n",
      "=== \n",
      "\n",
      "And selecting the **\u201cperfect features\u201d** is not straight forward and requires **experimenting** and **iteraction**.\n",
      "\n",
      "But when we have chosen how to describe our data, X, we are ready for the clustering algorithms.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# When we have our X\n",
      "# We can go try find our Y\n",
      "\n",
      " $$\n",
      "X = \n",
      "  \\begin{pmatrix} document_1 \\\\ document_2 \\\\ document_3 \\\\ document_4 \\\\ \\vdots \\\\ document_n \\end{pmatrix} \n",
      "$$\n",
      "<br>\n",
      " $$\n",
      "Y = \n",
      "  \\begin{pmatrix} subject_1 \\\\ subject_2 \\\\ subject_1 \\\\ subject_3 \\\\ \\vdots \\\\ subject_k \\end{pmatrix} \n",
      "$$\n",
      "\n",
      "#$document_1$ and $document_3$ both share $subject_1$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "If what we want to cluster is users and the relationship to find is who listens to the same music, X should be build up of features such as:\n",
      "\n",
      "Artist names, Track names, genres of artists and the quantities the number of occurrences. \n",
      "So if a user listen to a artist a lot our data will represent that. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Clustering algorithms\n",
      "* ### K-means\n",
      "* ### Hierarchical Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Different type of clustering algorithms based on the output. \n",
      "\n",
      "Hard clustering, soft clustering and hierarchical clustering.\n",
      "\n",
      "* Hard (flat) clustering as given by K-means give one subject to each document\n",
      "* Soft gives a distribution of subjects for each document (LDA, EM-algorithm)\n",
      "* Hierarchical gives a Tree of Hierarchical clusters - Not a one-to-one output!\n",
      "\n",
      "Also, the notion of similarity is important, as mentioned before <br>\n",
      "\n",
      "As we now only focus only **NUMERICAL** vectors we can use **Distance functions**\n",
      "as similarity. Euclidean, cosine etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\">K-means<h1>\n",
      "\n",
      "* Need to specify: K  (Number of clusters)\n",
      "* Each cluster is the _mean_ of the samples (centroids)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "* **Most popular**\n",
      "* partitions into K different clusters\n",
      "* And assigns objects to the cluster which has the closest mean"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## The algorithm:\n",
      "### Specify K clusters \n",
      "\n",
      "### 1. Randomly create k partitions\n",
      "### 3. Calculate centroid of partitions (with the mean)\n",
      "### 4. Associate samples to their closest center (with squared euclidean distance)\n",
      "### 5. Repeat until changes very little"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "* First sum: Each cluster\n",
      "* Second sum: Each observation\n",
      "* **Squred euclidean distance** to the mean and minimise it\n",
      "\n",
      "This works in two steps iteratively, \n",
      "* Assign each observation to the cluster which mean is closest \n",
      "* recalculate the arithmetic means based on the new assignments and do the assignment again.'\n",
      "\n",
      "Until convergence = centroids dont change anymore"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# These examples are stole. \n",
      "# Thank you Summer School in Statistics for Astronomers V!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Our example 2-D DATA\n",
      "<img src=\"fig/thedata.png\" style=\"height:400px; width:400px;\"></img>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# K = 2, RANDOM ASSIGNMENT\n",
      "<img src='fig/kmeans1.jpg' style=\"height:400px; width:400px;\"></img>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Update the centroids (Move clusters)\n",
      "<img src=\"fig/km2.jpg\" style=\"height:400px; width:400px;\"></img>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Recalculate what is the closest center (Update colors)\n",
      "<img src='fig/kmeans3.jpg' style=\"height:400px; width:400px;\" ></img>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Iterate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Update the centroids (Move clusters)\n",
      "<img src='fig/kmeans4.jpg' style=\"height:400px; width:400px;\"></img>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Recalculate what is the closest center (Update colors)\n",
      "\n",
      "<img src='fig/kmeans5.jpg' style=\"height:400px; width:400px;\"></img>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Stop when converge\n",
      "<img src='fig/kmeans6.jpg' style=\"height:400px; width:400px;\"></img>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$\\underset{\\mathbf{S}} {\\operatorname{arg\\,min}}  \\sum_{i=1}^{k} \\sum_{\\mathbf x \\in S_i} \\left\\| \\mathbf x - \\boldsymbol\\mu_i \\right\\|^2$$\n",
      "# Local optimum\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Note that the centroids can be arbitrary points in space and change just a little for each iteration.\n",
      "\n",
      "**can converge to a local optimum **\n",
      "**always will find a clustering but not guaranteed to be the best**\n",
      "\n",
      "The solution to this is to retry the algorithm mutiple times but with different initialisation of clusters. \n",
      "\n",
      "*(sensistive to outliers: the mean)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\">Hierarchical agglomerative clustering (HAC)<h1>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "No need to specify the number of clusters! Huge benefit!\n",
      "\n",
      "Works on distance matrixes =  We can choose what distance to use\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## The (agglomerative) algorithm:\n",
      "### Each sample is a cluster\n",
      "\n",
      "### Merge the two closest clusters until there is only one left\n",
      "\n",
      "### Output: Linkage matrix (How and when did we merge two clusters)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "The other hierarchical clustering is the opposite\n",
      "\n",
      "\n",
      "The output can be vizualised in a dendrogram.\n",
      "In the dendrogram, we can see the steps of the algorithm, from one cluster\n",
      "as each sample, to one big cluster!\n",
      "\n",
      "Whatch out, big complicated, colorful picture incoming!\n",
      "\n",
      "The bottom is the begining, the top the end"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Dendrogram\n",
      "<img src=\"fig/dendrogram.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "At the bottom of the dendrogram we see each sample as its own cluster and the higher we go the less clusters there are. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "\n",
      "These are not the only ones. Dbscan, k-medoids, Latent dirchlet allocation\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\"> Evaluation of the algorithm <h1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "We are getting to the end of the presentation - first evaluation and then machine learning at spotify, then we are done.\n",
      "\n",
      "But, what do we do when we have X and Y\u2019. \n",
      "\n",
      "There is no reward or error to evalute a solution.\n",
      "\n",
      "Usually, clustering is evaluated based on the shape of the clustering. \n",
      "\n",
      "This is called internal evaluation.\n",
      "\n",
      "We can compute some internal evaluation to compare\n",
      "\n",
      "different algorithms and parameter tuning (such as k in k-means).\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Silhouette score\n",
      "$$a(x) = \\text{average distance to all other sample in the SAME cluster} $$\n",
      "\n",
      "$$b(x) = \\text{average distance to the samples of the closest cluster}$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "\n",
      "One internal evaluation is the silhouette score which is a score s on each sample i, s(i).\n",
      "\n",
      "a(i) is defined as the average distance to all other sample in the SAME cluster \n",
      "\n",
      "and b(i) to the samples of the closest cluster. The silhouette score is \u2026. and range -1 <= s(i) <= 1\n",
      "\n",
      "-1 : i lies closer to another cluster, 1 : lies more closely to its cluster.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\n",
      "$$s(x) = \\frac{b(x) - a(x)}{\\max\\{a(x),b(x)\\}}, \\forall x\\in X$$\n",
      "\n",
      "$$ -1 \\le s(x) \\le 1 $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "## If we have training data we can use it to evaluate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Visualization with dimensionality reduction "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<img src=\"fig/pca.jpeg\"  />\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Not a focus of this talk but worth mentioning\n",
      "\n",
      "If we have high-dimensional data, which is impossible to vizualise\n",
      "\n",
      "We can construct a new data set with fewer conditions\n",
      "Where each conditions is a linear combinations of original conditions\n",
      "\n",
      "\n",
      "Helps us find dominant information in the data\n",
      "Assumption that the features with low variance contains small amount of information\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 style=\"font-size: 300%\">Machine Learning at Spotify <h1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "I want to finish of with mentioning what and where we do Machine Learning\n",
      "spotify.\n",
      "\n",
      "Most famously: Rec sys\n",
      "\n",
      "* Ensemble model\n",
      "    * Collaborative Filtering\n",
      "    * Deep learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Recommender systems\n",
      "* Discovery - new music\n",
      "* Related artists\n",
      "* Radio\n",
      "\n",
      "Rec sys: http://www.a1k0n.net/spotify/ml-madison/ <br>\n",
      "Deep Learning: http://benanne.github.io/2014/08/05/spotify-cnns.html\n",
      "\n",
      "\n",
      "# Clustering \n",
      "* User behavior\n",
      "* Artist disambiguation ( the kent bug )"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "We do cluster analysis on user behaviour - \n",
      "trying find different charecaristics of users \n",
      "and measure/analyze who is doing what.\n",
      "\n",
      "We have, what we refere to as \u201cthe Kent bug\u201d.\n",
      "So, appratenly artist names are not unique.\n",
      "There exists multiple artists with the same name hence it happens that we mix up artist pages\n",
      "and display the wrong kents tracks on the swedish Kents page. \n",
      "This problem is called Artist disambiguation \n",
      "and is essentially the same as author disambiguation \n",
      "( People with the same name, writing on different subjects).\n",
      "We solve it with hierarchical clustering where clusters are potentially ambigious artists \n",
      "and let humans separate them.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "## Stay put, we will soon start again!\n",
      "\n",
      "\n",
      "\n",
      ">*18:15 - 18:45:*\n",
      "Introduction to clustering and feature engineering\n",
      ">\n",
      ">** Short break, refill drinks, greet & meet etc. **\n",
      ">\n",
      ">*19:00 - 19:30:*\n",
      "Introduction to data (mining) analysis in Python\n",
      ">\n",
      ">*19:30: *\n",
      "Hang around and discuss clustering and machine learning. \n",
      ">\n",
      "\n",
      "\n",
      "<img src=\"fig/Spotify_Logo_RGB_Black.png\" style=\"height:auto; width:auto; max-width:300px; max-height:300px; position:left;\"/>\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    }
   ],
   "metadata": {}
  }
 ]
}